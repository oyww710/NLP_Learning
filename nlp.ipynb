{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Text as Data\n",
    "\n",
    "Why do we care about Natural Language Processing?  There is an *immense* amount of data that exists in text form and we would like to make sense of it just how we'd like to make sense of any other source of data.  If we completely ignore text, we are effectively throwing out valuable data that could be used to create standalone models or even be incorporated into a greater model that incorporates other data (think combining not only the rating a customer gave a product but also incorporating the comment they wrote about their experience).\n",
    "\n",
    "Natural Language Processing is a subfield of machine learning focused on making sense of text. Text is inherently unstructured but there are a variety of techniques for converting (vectorizing) text into a format that a machine learning algorithm can interpret.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "* **Token**: Words in a list.  Each document is a list of tokens.\n",
    "* **Document**: Text in a string.\n",
    "* **Corpus**: Collection of all documents you're interested in.\n",
    "* **Vocab**: Set of words.\n",
    "* **Vectorization**: Vector Representation of our list of words.\n",
    "* **Stop Words**: Words that don't matter\n",
    "\n",
    "---\n",
    "\n",
    "$d_0$: \"Clinton stuns crowd, scares viewers, with debate performance\"\n",
    "\n",
    "$d_1$: \"Viewers impressed by how male Trump looked during debate\"\n",
    "\n",
    "$d_2$: \"Trump and Clinton clash over Economy and Race Relations\"\n",
    "\n",
    "$d_3$: \"Impressed by debate, voters in York County, VA seriously ambivalent\"\n",
    "\n",
    "$d_4$: \"I am serious, and don't call me Shirley\"\n",
    "\n",
    "$d_5$: \"Breaking: Trump made it to debate in New York state\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "\n",
    "The first thing we need to do is process our text.  Common steps include:\n",
    "\n",
    "* Lower all of your text\n",
    "* Strip out misc. spacing and punctuation\n",
    "* Remove stop words\n",
    "    * Stop words are words which have no real meaning but make the sentence grammatically correct.  Words like 'am', 'the', 'my', 'to', & c.  NLTK contains 153 words for the English set of stop words\n",
    "    * These can also be domain specific.\n",
    "* Stem/Lemmatize our text\n",
    "    * The goal of this process is to transform a word into its base form.\n",
    "    * e.g. \"ran\", \"runs\" -> \"run\"\n",
    "    * You can think of the base form as what you would look up in a dictionary\n",
    "    * Popular techniques include stemming and lemmatization.  Stemming removes the suffix whereas Lemmatization attempt to change all forms of the word to the same form.  Stemmers tend to operate on a single word without knowledge of the overall context.\n",
    "    * These are not perfect, however (e.g. taking the lemma of \"Paris\" and getting \"pari\")\n",
    "* Part-Of-Speech Tagging\n",
    "* N-grams\n",
    "\n",
    "After running the following processing this is the resulting text:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import pattern.en as en\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def lemmatize_string(doc):\n",
    "    stop_words = stopwords.words('english')\n",
    "    doc = doc.lower().translate(None, punctuation)\n",
    "    return ' '.join([en.lemma(w) for w in doc.split() if w not in stop_words])\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    corpus = ['Clinton stuns crowd, scares viewers, with debate performance',\n",
    "              'Viewers impressed by how male Trump looked during debate',\n",
    "              'Trump and Clinton clash over Economy and Race Relations',\n",
    "              'Impressed by debate, voters in York County, VA seriously ambivalent',\n",
    "              \"I am serious, and don't call me Shirley\",\n",
    "              'Breaking: Trump made it to debate in New York state']\n",
    "    processed = map(lemmatize_string, corpus)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "$d_0$: 'clinton stun crowd scare viewer debate performance'\n",
    "\n",
    "$d_1$: 'viewer impress male trump look debate'\n",
    "\n",
    "$d_2$: 'trump clinton clash economy race relation'\n",
    "\n",
    "$d_3$: 'impress debate voter york county va seriously ambivalent'\n",
    "\n",
    "$d_4$: 'seriou dont call shirley'\n",
    "\n",
    "$d_5$: 'break trump make debate new york state'\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization\n",
    "\n",
    "### Term Frequency\n",
    "\n",
    "In order to utilize many of the machine learning algorithms we have learned, we must convert our text data into something that these algorithms can work with.  This is typically done by converting our corpus of text data into some form of numeric matrix representation.  The most simple form of numeric representation is called a [Term-Frequency](https://en.wikipedia.org/wiki/Document-term_matrix) matrix whereby each column of the matrix is a word, each row is a document, and each cell represents the count of that word in a document.\n",
    "\n",
    "**Note**: The matrix shown below is transposed from how it is typically represented in code (i.e. the documents will be rows with the tokens as columns).  Also, only a subset of the tokens are displayed in the table below.\n",
    "\n",
    "* $f_{t, d}$ = count of term $t$ in document $d$ where $t \\in T$ and $d \\in D$\n",
    "\n",
    "Token      | $d_0$ | $d_1$ | $d_2$ | $d_3$ | $d_4$ | $d_5$\n",
    "-----------|-------|-------|-------|-------|-------|------\n",
    "ambivalent | 0     | 0     | 0     | 1     | 0     | 0\n",
    "clinton    | 1     | 0     | 1     | 0     | 0     | 0\n",
    "trump      | 0     | 1     | 1     | 0     | 0     | 1\n",
    "impress    | 0     | 1     | 0     | 1     | 0     | 0\n",
    "debate     | 1     | 1     | 0     | 1     | 0     | 1\n",
    "serious    | 0     | 0     | 0     | 1     | 1     | 0\n",
    "break      | 0     | 0     | 0     | 0     | 0     | 1\n",
    "economy    | 0     | 0     | 1     | 0     | 0     | 0\n",
    "new        | 0     | 0     | 0     | 0     | 0     | 1\n",
    "york       | 0     | 0     | 0     | 1     | 0     | 1\n",
    "race       | 0     | 0     | 1     | 0     | 0     | 0\n",
    "shirley    | 0     | 0     | 0     | 0     | 1     | 0\n",
    "voter      | 0     | 0     | 0     | 1     | 0     | 0\n",
    "viewer     | 1     | 1     | 0     | 0     | 0     | 0\n",
    "\n",
    "* What problems do you see with this approach?\n",
    "\n",
    "\n",
    "One issue with this approach is due to the potential difference in document lengths.  A longer article that contains the complete transcript of a political debate is necessarily going to have larger values for the term counts than an article that is only a couple of sentences long.  This also serves to scale up the frequent terms and scales down the rare terms which are empirically more informative. We could normalize the term counts by the length of a document which would alleviate some of this problem.\n",
    "\n",
    "* L2 Normalization is the default in Sklearn\n",
    "\n",
    "$$tf(t, d) = \\frac{f_{t, d}}{\\sqrt{\\sum_{i \\in V} (f_{i, d})^2}}$$\n",
    "\n",
    "For example:\n",
    "\n",
    "Token      | $d_0$ | $d_1$ | $d_2$ | $d_3$ | $d_4$ | $d_5$\n",
    "-----------|-------|-------|-------|-------|-------|------\n",
    "ambivalent | 0     | 0     | 0     | $\\frac{1}{\\sqrt{6}}$     | 0     | 0\n",
    "clinton    | $\\frac{1}{\\sqrt{3}}$ | 0 | $\\frac{1}{\\sqrt{4}}$    | 0     | 0     | 0\n",
    "trump      | 0 | $\\frac{1}{\\sqrt{4}}$ | $\\frac{1}{\\sqrt{4}}$ | 0 | 0  | $\\frac{1}{\\sqrt{5}}$\n",
    "\n",
    "\n",
    "But we can go further and have the value associated with a document-term be a measure of the importance in relation to the rest of the corpus.  We can achieve this by creating a [Term-Frequency Inverse-Document-Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (TF-IDF) matrix.  This is done by multiplying the Term-Frequency by a statistic called the Inverse-Document-Frequency, which is a measure of how much information a word provides (i.e. it is a measure of whether a term is common or rare across all documents).\n",
    "\n",
    "$$idf(t, D) = log \\frac{|D|}{|\\{d \\in D: t \\in d\\}| + 1}$$\n",
    "\n",
    "For example:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "idf('ambivalent', D) &= log \\frac{6}{1 + 1} &= 1.099 \\\\\n",
    "idf('clinton', D) &= log \\frac{6}{2 + 1} &= 0.693 \\\\\n",
    "idf('trump', D) &= log \\frac{6}{3 + 1} &= 0.405 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This rational for using the logarithmic scale being that a term that occurs 10 times more than another isn't 10 times more important than it.  The 1 term on the bottom is known as a smoothing constant and is there to ensure that we don't have a zero in the denominator.\n",
    "\n",
    "* Why do we need the smoothing constant?\n",
    "\n",
    "The end result then is thus...\n",
    "\n",
    "$$tfidf(t, d, D) = tf(t, d) \\cdot idf(t, D)$$\n",
    "\n",
    "For example:\n",
    "\n",
    "$$tfidf('ambivalent', d_3, D) = \\frac{1}{\\sqrt{6}} \\cdot \\frac{6}{1 + 1} = 0.449$$\n",
    "\n",
    "Token      | $d_0$ | $d_1$ | $d_2$ | $d_3$ | $d_4$ | $d_5$\n",
    "-----------|-------|-------|-------|-------|-------|------\n",
    "ambivalent | 0     | 0     | 0     | 0.449 | 0     | 0\n",
    "clinton    | 0.400 | 0     | 0.347 | 0     | 0     | 0\n",
    "trump      | 0     | 0.203 | 0.203 | 0     | 0     | 0.181\n",
    "\n",
    "What does this intuitively tell us?  What does a high score mean?  Roughly speaking a $tfidf$ score is an attempt to identify the most important words in a document.  If a word appears a lot in a particular document it will get a high $tf$ score.  But if a word also appears in every other document in your corpus, it clearly doesn't convey anything unique about what that document is about.  Thus, a term will get a high score if it occurs many times in a document and appears in a small fraction of the corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF in Scikit-Learn\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "```\n",
    "\n",
    "Sklearn provides a convenient manner for creating this matrix.  Some of the arguments to note are:\n",
    "\n",
    "* `max_df`\n",
    "    * Can either be absolute counts or a between 0 and 1 indicating a proportion.  Specifies words which should be excluded due to appearing in more than a given number of documents.\n",
    "* `min_df`\n",
    "    * Can either be absolute counts or a between 0 and 1 indicating a proportion.  Specifies words which should be excluded due to appearing in less than a given number of documents.\n",
    "* `max_features`\n",
    "    * Specifies the number of features to include in the resulting matrix.  If not `None`, build a vocabulary that only considers the top `max_features` ordered by term frequency across the corpus.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarity\n",
    "\n",
    "Now that we have a matrix representation of our corpus, how should we go about comparing documents to identify those which are most similar to one another?\n",
    "\n",
    "* What are some metrics that you can think of and why might you prefer one over the other?\n",
    "\n",
    "* Cosine Similarity\n",
    "\n",
    "$$similarity = \\frac{A \\cdot B}{\\lVert A \\rVert \\lVert B \\rVert} = \\frac{\\sum_{i=1}^n A_i B_i}{\\sqrt{\\sum_{i=1}^n A_i^2} \\sqrt{\\sum_{i=1}^n B_i^2}}$$\n",
    "\n",
    "* Euclidean Distance\n",
    "\n",
    "$$d(A, B) = \\sqrt{\\sum_{i=1}^n (A_i - B_i)^2}$$\n",
    "\n",
    "```python\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Examples\n",
    "\n",
    "Let's go over a simple example.\n",
    "\n",
    "Sample sentence:\n",
    "\n",
    "```\n",
    "I banked on going to the river bank today.\n",
    "```\n",
    "\n",
    "Let's start with the flow:\n",
    "\n",
    "```\n",
    "Tokenization ----> Sentence segmentation ----> Stemming/Lemmatization ---->\n",
    "stop words ----> Bag of words/TFIDF\n",
    "```\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "```python\n",
    "['I', 'banked', 'on', 'going', 'to', 'the', 'river', 'bank', 'today', '.']\n",
    "```\n",
    "\n",
    "#### Document Creation\n",
    "\n",
    "Remember: a document is a list of lists where each list is a list of strings\n",
    "that contains one token.\n",
    "\n",
    "```python\n",
    "[['I', 'banked', 'on', 'going', 'to', 'the', 'river', 'bank', 'today', '.']]\n",
    "```\n",
    "\n",
    "#### Lower case\n",
    "\n",
    "```python\n",
    "[['i', 'banked', 'on', 'going', 'to', 'the', 'river', 'bank', 'today', '.']]\n",
    "```\n",
    "\n",
    "#### Stemming/Lemmatization\n",
    "\n",
    "```python\n",
    "[['i', 'bank', 'on', 'go', 'to', 'the', 'river', 'bank', 'today', '.']]\n",
    "```\n",
    "\n",
    "#### Remove stop words\n",
    "\n",
    "```python\n",
    "[['bank', 'go', 'river', 'bank', 'today', '.']]\n",
    "```\n",
    "\n",
    "#### Information Retrieval\n",
    "\n",
    "IR is a subfield of natural language processing used for understanding similar documents. We can think of similar documents as a web search problem.\n",
    "\n",
    "Whenever you search a website, tfidf is what a lot of websites to do bring up relevant web pages for a query.\n",
    "\n",
    "TFIDF is a relevance measure. It is used for identifying documents that are related to a search query.\n",
    "\n",
    "A search query itself is also a document.\n",
    "\n",
    "If we remember documents are word counts based on a vocab. Search queries get encoded in the same way and then compared against the search engine to identify and rank candidate search results.\n",
    "\n",
    "Documents are stored in a data structure called an inverted index. An inverted index is a reverse store of words to documents.\n",
    "\n",
    "TO build an inverted index first a forward index mapping words to document occurrences is built.\n",
    "\n",
    "In this case, it would look something like:\n",
    "\n",
    "```\n",
    "T[0] = \"it is what it is\"\n",
    "T[1] = \"what is it\"\n",
    "T[2] = \"it is a banana\"\n",
    "```\n",
    "\n",
    "This process is encapsulated by our tokenization process from before. Basically, we store full text documents and occurrences in a database. We use this to build an inverted index which we then do seaerch queries against.\n",
    "\n",
    "This will look something like:\n",
    "\n",
    "```\n",
    "\"a\":      {2}\n",
    "\"banana\": {2}\n",
    "\"is\":     {0, 1, 2}\n",
    "\"it\":     {0, 1, 2}\n",
    "\"what\":   {0, 1}\n",
    "```\n",
    "\n",
    "Note that the inverted index is composed of our vocab.\n",
    "\n",
    "Let's remind ourselves of the tfidf formula and dive in to what it means to be a relevance measure.\n",
    "\n",
    "In a document d with term t:\n",
    "\n",
    "```\n",
    "tfidf(t,d) = tf(t,d) * idf(t)\n",
    "```\n",
    "\n",
    "Concretely, we take a document from the postings list (and even search queries!)\n",
    "\n",
    "and based on the term frequencies and document frequencies compute a score for each term.\n",
    "\n",
    "We then use this for identifying documents relevant to a search query.\n",
    "\n",
    "Concretely:\n",
    "\n",
    "```\n",
    "idf(t, D) = log(N / (1 + number of documents containing term t ))\n",
    "```\n",
    "\n",
    "where N is the number of documents in the corpus.\n",
    "\n",
    "idf is again the amount of information an individual word provides.\n",
    "\n",
    "Let's run through how this works using our inverted indices.\n",
    "\n",
    "### Documents\n",
    "\n",
    "1. I am teaching at BitTiger.\n",
    "2. There are students learning at BitTiger.\n",
    "3. Students teach each other at BitTiger.\n",
    "4. Students are learning from other students.\n",
    "5. Students are learning data science.\n",
    "\n",
    "The first step is to remove stop words, stem and lowercase. Now we can calculate the Term Frequency.\n",
    "\n",
    "**Term Frequency (tf)**\n",
    "\n",
    "| document | teach | bittiger | student | learn | data | science |\n",
    "|:-------- | -----:| --------:| -------:| -----:| ----:| ------: |\n",
    "|        1 |     1 |        1 |       0 |     0 |    0 |       0 |     \n",
    "|        2 |     0 |        1 |       1 |     1 |    0 |       0 |\n",
    "|        3 |     1 |        1 |       1 |     0 |    0 |       0 |\n",
    "|        4 |     0 |        0 |       2 |     1 |    0 |       0 |\n",
    "|        5 |     0 |        0 |       1 |     1 |    1 |       1 |\n",
    "\n",
    "This is our document table. Each document has word counts. Let's compute some tfidf scores.\n",
    "\n",
    "```\n",
    "tfidf(t, d) = tf(t, d) * idf(t)\n",
    "idf(t, D) = log(N / (1 + number of documents containing term t) )\n",
    "```\n",
    "\n",
    "**Inverse Document Frequency (idf)**\n",
    "\n",
    "\n",
    "| word      | df |                 idf |     idf |\n",
    "| --------- | -- | ------------------- | ------- |\n",
    "| teach     |  2 | `log (5 / (1 + 2))` | 0.51083 |\n",
    "| bittiger  |  3 | `log (5 / (1 + 3))` | 0.22314 |\n",
    "| student   |  4 | `log (5 / (1 + 4))` |       0 |\n",
    "| learn     |  3 | `log (5 / (1 + 3))` | 0.22314 |\n",
    "| data      |  1 | `log (5 / (1 + 1))` | 0.91629 |\n",
    "| science   |  1 | `log (5 / (1 + 1))` | 0.91629 |\n",
    "\n",
    "\n",
    "After formula applications:\n",
    "\n",
    "  \n",
    " **Term Frequency - Inverse Document Frequency (tfidf)**\n",
    "\n",
    "| document |   teach | bittiger | student |   learn |    data | science |\n",
    "|:-------- | -------:| --------:| -------:| -------:| -------:| ------: |\n",
    "|        1 | 0.51083 |  0.22314 |     0.0 |     0.0 |       0 |       0 |\n",
    "|        2 |     0.0 |  0.22314 |     0.0 | 0.22314 |       0 |       0 |\n",
    "|        3 | 0.51083 |  0.22314 |     0.0 |     0.0 |       0 |       0 |\n",
    "|        4 |     0.0 |      0.0 |     0.0 | 0.22314 |       0 |       0 |\n",
    "|        5 |     0.0 |      0.0 |     0.0 | 0.22314 | 0.91629 | 0.91629 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
